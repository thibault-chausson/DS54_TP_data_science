{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Gathering Web Data\n",
    "\n",
    "1. Data gathering web data using cURL\n",
    "\n",
    "One of the most important tools for gathering data from the web is cURL. It's a command line utility that allows you to request and download a webpage or other data over HTTP, like images or documents. cURL is often scripted to scrape web data from various sources or used as part of a web crawler to index pages.\n",
    "Bring up the webpage, top5ofanything.com and look at a list of data that contains the Top 5 Most Reliable Car Brands for 2021 : https://top5ofanything.com/list/5b6d4671/The-Top-5-Most-Reliable- Car-Brands-for-2021\n",
    "In a folder called curl-demo, run the command curl (Use sudo apt install curl to install it if required). direct it into a file and save it (which file type should you use?)\n",
    "use curl --head and then the URL that shows header information. The important thing is that you shuld get a status code 200 as a valid request\n",
    "to save on bandwidth if you are downloading a lot of data (a lot of pages), you can use the compressed option : use the command, curl --compressed –head and then the URL. the content encoding should come back as gzip\n",
    "to search an element, you can use the following command: curl –data ‘’q=France’’ and then the url/search that it's posting to (in double quotes, you give the form element q that is the search element). This is similar to top5ofanything.com/search on the browser, you will get the same results from that posted data (to check).\n",
    "executes the command, `curl --data-urlencode \"q=highest mountains\" https://top5ofanything.com/search/` and comments the option urlencode.\n",
    "cURL provides cookie handling. It’s information that gets passed back and forth between the browser and the web server. To store the cookies in file cookies.txt, check the following command:\n",
    "`curl --cookie-jar cookies.txt --output output.html https://www.google.com` examines the file cookies.txt (using command cat or more).\n",
    "\n",
    "2. Extracting Spreadsheet Data with in2csv\n",
    "\n",
    "Having data in Microsoft Excel format is very common, but this is not always a good format for data processing. We'd like tabular data like CSV.\n",
    "use in2csv to convert an Excel spreadsheet data to CSV format. In a folder in2csv-demo, get an excel file , e.g., https://www.itu.int/ITU-D/ict/statistics/material/excel/EstimatedInternetUsers00-09.xls, which is Estimated based on urban/rural distribution of Internet users. Then use in2csv which is a Python utility that comes part of the CSV kit library from Python (install csvkit first using the pip package manager). Pipe the command into grep, to do grep for France. Save in a csv file and check it.\n",
    "\n",
    "3. Extracting Spreadsheet Data with Agate\n",
    "\n",
    "If we use Python for scripting, then extracting spreadsheet data is best done with the Agate library. Agate is a general-purpose data analysis library that can be used for data wrangling and other data science tasks.\n",
    "Write out a script xls2csv.py on extracting the spreadsheet data with the Python Agate library.\n",
    "\n",
    "4. Extracting HTML Data using Python and BeautifulSoup\n",
    "\n",
    "If your data science needs require that you extract web data from HTML documents, you'll need to be able to parse and extract HTML tags from documents.\n",
    "Write out a script using the Python package BeautifulSoup to download and extract HTML tags from the web.\n",
    "\n",
    "5. Work with metadata in email headers\n",
    "\n",
    "write out a script parse-email.py to parse a source file from your email server using a Python library for email message parsing.\n",
    "\n",
    "6. Connecting to Remote Data\n",
    "\n",
    "to connect directly to a remote server that hosts your data, the most frequently used utility to connect to remote systems is Secure Shell, or SSH.\n",
    "On a trusted computer, you can copy your SSH keys to the remote computer so that it trusts you implicitly (without password prompt except when you install it first time). Check the usage of the command ssh-copy-id to have a passwordless connection that uses a key-based authentication.\n",
    "\n",
    "7. Copying Remote Data\n",
    "\n",
    "To copy remote data using a secure copy, the command secure copy or SCP for short is used to copy a remote file securely using key-based authentication.\n",
    "Explain how scp can be used with key-based authentication\n",
    "\n",
    "8. Synchronizing Remote Data\n",
    "\n",
    "Keeping remote data synchronized between devices, for example from a server to a single workstation can be complex. You can use rsync for synchronizing remote data (with ssh).\n",
    "You could be using rsync between data in directories on a single computer. For instance, you can synchronize copies across different mapped drives or storage media.\n",
    "Check rsync usage and answer this question: What is difference between scp and rsync?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    " 1. Data gathering web data using cURL\n",
    "\n",
    "`curl --data-urlencode \"q=highest mountains\" https://top5ofanything.com/search/`\n",
    "\n",
    "Le résultat de cette requête est le suivant :\n",
    "\n",
    "```html\n",
    "<!doctype html><html lang=en><head><meta charset=utf-8>\n",
    "<script src=\"https://privacy.gatekeeperconsent.com/tcf2_stub.js\" data-cfasync=\"false\"></script><script>window.__ezWillLoadCnx=1;</script><script data-ezscrex=false data-cfasync=false data-pagespeed-no-defer>var __ez=__ez||{};__ez.stms=Date.now();__ez.evt={};\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 2. Extracting Spreadsheet Data with in2csv\n",
    "import csvkit as csv\n",
    "import paramiko as paramiko"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "XLRDError",
     "evalue": "No sheet named <'sheet'>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/DS54_TP_data_science/lib/python3.8/site-packages/xlrd/book.py:466\u001B[0m, in \u001B[0;36mBook.sheet_by_name\u001B[0;34m(self, sheet_name)\u001B[0m\n\u001B[1;32m    465\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 466\u001B[0m     sheetx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sheet_names\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43msheet_name\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
      "\u001B[0;31mValueError\u001B[0m: 'sheet' is not in list",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mXLRDError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 18\u001B[0m\n\u001B[1;32m     15\u001B[0m input_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./in2csv-demo/EstimatedInternetUsers00-09.xls\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Replace with the path to your input spreadsheet\u001B[39;00m\n\u001B[1;32m     16\u001B[0m output_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./in2csv-demo/agate.csv\u001B[39m\u001B[38;5;124m\"\u001B[39m  \u001B[38;5;66;03m# Replace with the desired path for the output CSV file\u001B[39;00m\n\u001B[0;32m---> 18\u001B[0m \u001B[43mxls_to_csv\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_file\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[3], line 8\u001B[0m, in \u001B[0;36mxls_to_csv\u001B[0;34m(input_file, output_file)\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mxls_to_csv\u001B[39m(input_file, output_file):\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;66;03m# Load the spreadsheet using agate\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m     table \u001B[38;5;241m=\u001B[39m \u001B[43magate\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_xls\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msheet\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43msheet\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;66;03m# Write the data to a CSV file\u001B[39;00m\n\u001B[1;32m     11\u001B[0m     table\u001B[38;5;241m.\u001B[39mto_csv(output_file)\n",
      "File \u001B[0;32m~/miniconda3/envs/DS54_TP_data_science/lib/python3.8/site-packages/agateexcel/table_xls.py:76\u001B[0m, in \u001B[0;36mfrom_xls\u001B[0;34m(cls, path, sheet, skip_lines, header, encoding_override, row_limit, **kwargs)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, sheet \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(sheets):\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sheet, six\u001B[38;5;241m.\u001B[39mstring_types):\n\u001B[0;32m---> 76\u001B[0m         sheet \u001B[38;5;241m=\u001B[39m \u001B[43mbook\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msheet_by_name\u001B[49m\u001B[43m(\u001B[49m\u001B[43msheet\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     77\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(sheet, \u001B[38;5;28mint\u001B[39m):\n\u001B[1;32m     78\u001B[0m         sheet \u001B[38;5;241m=\u001B[39m book\u001B[38;5;241m.\u001B[39msheet_by_index(sheet)\n",
      "File \u001B[0;32m~/miniconda3/envs/DS54_TP_data_science/lib/python3.8/site-packages/xlrd/book.py:468\u001B[0m, in \u001B[0;36mBook.sheet_by_name\u001B[0;34m(self, sheet_name)\u001B[0m\n\u001B[1;32m    466\u001B[0m     sheetx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sheet_names\u001B[38;5;241m.\u001B[39mindex(sheet_name)\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m--> 468\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m XLRDError(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mNo sheet named <\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m>\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m%\u001B[39m sheet_name)\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msheet_by_index(sheetx)\n",
      "\u001B[0;31mXLRDError\u001B[0m: No sheet named <'sheet'>"
     ]
    }
   ],
   "source": [
    "# 3. Extracting Spreadsheet Data with Agate\n",
    "import agate\n",
    "\n",
    "def xls_to_csv(input_file, output_file):\n",
    "    # Load the spreadsheet using agate\n",
    "    table = agate.Table.form_xls(input_file)\n",
    "\n",
    "    # Write the data to a CSV file\n",
    "    table.to_csv(output_file)\n",
    "\n",
    "    print(f\"Data successfully extracted from {input_file} and saved as {output_file}.\")\n",
    "\n",
    "input_file = \"./in2csv-demo/EstimatedInternetUsers00-09.xls\"  # Replace with the path to your input spreadsheet\n",
    "output_file = \"./in2csv-demo/agate.csv\"  # Replace with the desired path for the output CSV file\n",
    "\n",
    "xls_to_csv(input_file, output_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T14:21:16.743099Z",
     "start_time": "2023-05-15T14:21:16.706179Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "./\n",
      "Accueil\n",
      "./\n",
      "Nos locations\n",
      "./locations.php\n",
      "Nos activités\n",
      "./activites.php\n",
      "Nos photos\n",
      "./photos.php?annee=2023\n",
      "Vos avis\n",
      "./avis.php?annee=2023\n",
      "À-propos\n",
      "./a_propos.php\n",
      "Dossier d'inscription >\n",
      "Uploads/_062822000840.pdf\n",
      "En savoir plus >\n",
      "locations.php\n",
      "\n",
      "locations.php\n",
      "Nous admirer >\n",
      "photos.php\n",
      "\n",
      "photos.php\n",
      "\n",
      "photos.php\n",
      "\n",
      "photos.php\n",
      "\n",
      "photos.php\n",
      " \n",
      "activites.php#comp\n",
      " La compétition \n",
      "activites.php#comp\n",
      " \n",
      "activites.php#loi\n",
      " La rivière \n",
      "activites.php#loi\n",
      " \n",
      "activites.php#pisc\n",
      " La piscine \n",
      "activites.php#pisc\n",
      "FaceBook\n",
      "https://www.facebook.com/profile.php?id=100040638682931\n",
      "Envoyez-nous un e-mail !\n",
      "mailto:rcck.ardennes@gmail.com\n",
      "Nous trouver\n",
      "https://www.google.com/maps/dir//rethel+chateau+canoe+kayak/data=!4m6!4m5!1m1!4e2!1m2!1m1!1s0x47e98907bf9e2ad3:0x377eff8f93d189d4?sa=X&ved=2ahUKEwjBgMmx8tz1AhWD3YUKHZYYC7IQ9Rd6BAg5EAQ\n",
      "Site map\n",
      "./site-map.php\n"
     ]
    }
   ],
   "source": [
    "# 4. Extracting HTML Data using Python and BeautifulSoup\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL du site web à extraire\n",
    "url = \"https://rcck-ardennes.fr\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.content\n",
    "\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    links = soup.find_all(\"a\")\n",
    "\n",
    "    for link in links:\n",
    "        print(link.text)\n",
    "        print(link.get(\"href\"))\n",
    "\n",
    "else:\n",
    "    print(\"Erreur lors de la requête HTTP : \", response.status_code)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T14:29:41.037766Z",
     "start_time": "2023-05-15T14:29:40.078456Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: Rappel - Recall\n",
      "Sender: Jaafar Gaber <jaafar.gaber@utbm.fr>\n",
      "Recipients: DS54 <DS54@utbm.fr>\n",
      "Date: Mon, 15 May 2023 11:41:28 +0200 (CEST)\n"
     ]
    }
   ],
   "source": [
    "# 5. Work with metadata in email headers\n",
    "import email\n",
    "from email.header import decode_header\n",
    "\n",
    "def parse_email(file_path):\n",
    "    with open(file_path, 'rb') as file:\n",
    "        message = email.message_from_binary_file(file)\n",
    "\n",
    "    # Extract metadata from email headers\n",
    "    subject = decode_header(message['Subject'])[0][0]\n",
    "    sender = decode_header(message['From'])[0][0]\n",
    "    recipients = decode_header(message['To'])[0][0]\n",
    "    date = message['Date']\n",
    "\n",
    "    print(f\"Subject: {subject}\")\n",
    "    print(f\"Sender: {sender}\")\n",
    "    print(f\"Recipients: {recipients}\")\n",
    "    print(f\"Date: {date}\")\n",
    "\n",
    "    # Additional header fields can be extracted in a similar manner\n",
    "    # For example: message['Cc'], message['Bcc'], message['Message-ID'], etc.\n",
    "\n",
    "# Example usage\n",
    "file_path = './email/email.eml'\n",
    "parse_email(file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T14:35:45.443335Z",
     "start_time": "2023-05-15T14:35:45.411039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thibaultchausson/miniconda3/envs/DS54_TP_data_science/lib/python3.8/site-packages/paramiko/client.py:343: ResourceWarning: unclosed <socket.socket fd=79, family=AddressFamily.AF_INET6, type=SocketKind.SOCK_STREAM, proto=0, laddr=('::1', 52231, 0, 0)>\n"
     ]
    },
    {
     "ename": "NoValidConnectionsError",
     "evalue": "[Errno None] Unable to connect to port 22 on 127.0.0.1 or ::1",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNoValidConnectionsError\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 15\u001B[0m\n\u001B[1;32m     12\u001B[0m client\u001B[38;5;241m.\u001B[39mset_missing_host_key_policy(paramiko\u001B[38;5;241m.\u001B[39mAutoAddPolicy())\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 15\u001B[0m     \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhostname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mport\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43musername\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpassword\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     stdin, stdout, stderr \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mexec_command(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mls -l\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     17\u001B[0m     output \u001B[38;5;241m=\u001B[39m stdout\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/miniconda3/envs/DS54_TP_data_science/lib/python3.8/site-packages/paramiko/client.py:368\u001B[0m, in \u001B[0;36mSSHClient.connect\u001B[0;34m(self, hostname, port, username, password, pkey, key_filename, timeout, allow_agent, look_for_keys, compress, sock, gss_auth, gss_kex, gss_deleg_creds, gss_host, banner_timeout, auth_timeout, gss_trust_dns, passphrase, disabled_algorithms)\u001B[0m\n\u001B[1;32m    362\u001B[0m     \u001B[38;5;66;03m# Make sure we explode usefully if no address family attempts\u001B[39;00m\n\u001B[1;32m    363\u001B[0m     \u001B[38;5;66;03m# succeeded. We've no way of knowing which error is the \"right\"\u001B[39;00m\n\u001B[1;32m    364\u001B[0m     \u001B[38;5;66;03m# one, so we construct a hybrid exception containing all the real\u001B[39;00m\n\u001B[1;32m    365\u001B[0m     \u001B[38;5;66;03m# ones, of a subclass that client code should still be watching for\u001B[39;00m\n\u001B[1;32m    366\u001B[0m     \u001B[38;5;66;03m# (socket.error)\u001B[39;00m\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(errors) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(to_try):\n\u001B[0;32m--> 368\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m NoValidConnectionsError(errors)\n\u001B[1;32m    370\u001B[0m t \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_transport \u001B[38;5;241m=\u001B[39m Transport(\n\u001B[1;32m    371\u001B[0m     sock,\n\u001B[1;32m    372\u001B[0m     gss_kex\u001B[38;5;241m=\u001B[39mgss_kex,\n\u001B[1;32m    373\u001B[0m     gss_deleg_creds\u001B[38;5;241m=\u001B[39mgss_deleg_creds,\n\u001B[1;32m    374\u001B[0m     disabled_algorithms\u001B[38;5;241m=\u001B[39mdisabled_algorithms,\n\u001B[1;32m    375\u001B[0m )\n\u001B[1;32m    376\u001B[0m t\u001B[38;5;241m.\u001B[39muse_compression(compress\u001B[38;5;241m=\u001B[39mcompress)\n",
      "\u001B[0;31mNoValidConnectionsError\u001B[0m: [Errno None] Unable to connect to port 22 on 127.0.0.1 or ::1"
     ]
    }
   ],
   "source": [
    "# 6. Connecting to Remote Data\n",
    "import paramiko\n",
    "\n",
    "hostname = ''\n",
    "port = 22\n",
    "username = ''\n",
    "password = ''\n",
    "\n",
    "\n",
    "client = paramiko.SSHClient()\n",
    "\n",
    "client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "\n",
    "try:\n",
    "    client.connect(hostname, port, username, password)\n",
    "    stdin, stdout, stderr = client.exec_command('ls -l')\n",
    "    output = stdout.read().decode('utf-8')\n",
    "    print(output)\n",
    "\n",
    "finally:\n",
    "    client.close()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-05-15T14:43:42.284422Z",
     "start_time": "2023-05-15T14:43:42.261207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 7. Copying Remote Data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 8. Synchronizing Remote Data"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
